{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import one_hot\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from torch.utils.data.sampler import SequentialSampler\n",
    "from transformers import BertModel, BertTokenizer, BertConfig\n",
    "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "# from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n",
    "from tokenizers import BertWordPieceTokenizer, ByteLevelBPETokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    work_path = os.path.dirname(os.path.abspath('.'))\n",
    "    input_path = os.path.join(work_path, \"input\")\n",
    "    data_path = os.path.join(input_path, \"tweet-sentiment-extraction\")\n",
    "    train_path = os.path.join(data_path, 'train.csv')\n",
    "    test_path = os.path.join(data_path, 'test.csv')\n",
    "    sample_submission_path = os.path.join(data_path, 'sample_submission.csv')\n",
    "    model_path = os.path.join(work_path, \"model\")\n",
    "    for path in [model_path]:\n",
    "        if not os.path.isdir(path):\n",
    "            os.makedirs(path)\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#     暂时没有截取，有很多符号的，长度不止110\n",
    "#     max_seq_len = 110\n",
    "    max_seq_len = 128\n",
    "    n_splits = 5\n",
    "    patience_epoch = 2\n",
    "    \n",
    "    batch_size = 32\n",
    "    epochs_num = 2\n",
    "    train_print_step = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class model_config:\n",
    "# model_name = 'bert'\n",
    "#     pretrain_model_name = 'bert-base-uncased'\n",
    "#     pretrain_model_path = os.path.join(config.input_path, pretrain_model_name)\n",
    "#     tokenizer = BertWordPieceTokenizer('{}/vocab.txt'.format(pretrain_model_path), lowercase=True)\n",
    "#     learning_rate = 1e-5\n",
    "#     adjust_lr_num = 0\n",
    "\n",
    "class model_config:\n",
    "    model_name = 'roberta'\n",
    "    pretrain_model_name = 'roberta-base'\n",
    "    pretrain_model_path = os.path.join(config.input_path, pretrain_model_name)\n",
    "    tokenizer = ByteLevelBPETokenizer(\n",
    "        vocab_file='{}/vocab.json'.format(pretrain_model_path), \n",
    "        merges_file='{}/merges.txt'.format(pretrain_model_path), \n",
    "        lowercase=True,\n",
    "        add_prefix_space=True\n",
    "    )\n",
    "    learning_rate = 1e-5  \n",
    "    adjust_lr_num = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed():\n",
    "    np.random.seed(0)\n",
    "    torch.manual_seed(0)\n",
    "    torch.cuda.manual_seed_all(0)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    \n",
    "def jaccard(str1, str2):\n",
    "    a = set(str1.lower().split())\n",
    "    b = set(str2.lower().split())\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "\n",
    "def get_selected_text(text, sentiment, start_idx, end_idx, offsets):\n",
    "    # Set the predicted output as the original tweet when the tweet's sentiment is \"neutral\",\n",
    "    # or the tweet only contains one word\n",
    "    if sentiment == \"neutral\" or len(text.split()) < 2:\n",
    "        selected_text = text\n",
    "        return selected_text\n",
    "    selected_text = \"\"\n",
    "    for ix in range(start_idx, end_idx + 1):\n",
    "        selected_text += text[offsets[ix][0]: offsets[ix][1]]\n",
    "        if (ix + 1) < len(offsets) and offsets[ix][1] < offsets[ix + 1][0]:\n",
    "            selected_text += \" \"\n",
    "    return selected_text\n",
    "\n",
    "\n",
    "def calculate_jaccard_score(text, true_selected_text, sentiment, pred_start_idx, pred_end_idx, offsets):\n",
    "    \"\"\"\n",
    "    Calculate the jaccard score from the predicted span and the actual span for a batch of tweets\n",
    "    \"\"\"\n",
    "    # A span's end index has to be greater than or equal to the start index\n",
    "    # If this doesn't hold, the start index is set to equal the end index (the span is a single token)\n",
    "    if pred_end_idx < pred_start_idx:\n",
    "        pred_end_idx = pred_start_idx\n",
    "    pred_selected_text = get_selected_text(text, sentiment, pred_start_idx, pred_end_idx, offsets)\n",
    "    score = jaccard(true_selected_text.strip(), pred_selected_text.strip())\n",
    "    return score\n",
    "\n",
    "\n",
    "def get_score(y_true_list, y_pred_list):\n",
    "    jaccard_score = 0\n",
    "    for y_true, y_pred in zip(y_true_list, y_pred_list):\n",
    "        text = y_true[0]\n",
    "        selected_text = y_true[1]\n",
    "        sentiment = y_true[2]\n",
    "        offsets = y_true[3]\n",
    "        pred_start_idx = y_pred[0]\n",
    "        pred_end_idx = y_pred[1]\n",
    "\n",
    "        score = calculate_jaccard_score(text, selected_text, sentiment, pred_start_idx, pred_end_idx, offsets)\n",
    "        jaccard_score += score\n",
    "    return jaccard_score / len(y_true_list)\n",
    "\n",
    "\n",
    "\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "\n",
    "    def __init__(self, df, tokenizer, mode='train'):\n",
    "        self.mode = mode\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        # self.x_data = []\n",
    "        # self.y_data = []\n",
    "        # for i, row in df.iterrows():\n",
    "        #     x, y = self.row_to_tensor(self.tokenizer, row)\n",
    "        #     self.x_data.append(x)\n",
    "        #     self.y_data.append(y)\n",
    "        self.data = []\n",
    "        for i, row in df.iterrows():\n",
    "            data = self.row_to_tensor(row)\n",
    "            try:\n",
    "                assert len(data[\"ids\"]) == config.max_seq_len\n",
    "                assert len(data['attention_mask']) == config.max_seq_len\n",
    "                assert len(data[\"token_type_ids\"]) == config.max_seq_len\n",
    "                assert len(data[\"offsets\"]) == config.max_seq_len\n",
    "            except:\n",
    "                print(data)\n",
    "                print('ids:{}'.format(len(data[\"ids\"])))\n",
    "                print('attention_mask:{}'.format(len(data[\"attention_mask\"])))\n",
    "                print('token_type_ids:{}'.format(len(data[\"token_type_ids\"])))\n",
    "                print('offsets:{}'.format(len(data[\"offsets\"])))\n",
    "            # tmp = {\n",
    "            #     'ids': torch.tensor(data[\"ids\"], dtype=torch.long),\n",
    "            #     'mask': torch.tensor(data[\"mask\"], dtype=torch.long),\n",
    "            #     'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long),\n",
    "            #     'targets_start': torch.tensor(data[\"targets_start\"], dtype=torch.long),\n",
    "            #     'targets_end': torch.tensor(data[\"targets_end\"], dtype=torch.long),\n",
    "            #     'sentiment': data[\"sentiment\"],\n",
    "            #     'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "            # }\n",
    "            if self.mode in ['train', 'val']:\n",
    "                start_idx, end_idx = self.get_target_idx(row, data['tweet'], data['offsets'])\n",
    "                data['selected_text'] = row['selected_text']\n",
    "                data['start_idx'] = start_idx\n",
    "                data['end_idx'] = end_idx\n",
    "            data['sentiment'] = row['sentiment']\n",
    "            self.data.append(data)\n",
    "\n",
    "    def get_target_idx(self, row, tweet, offsets):\n",
    "        selected_text = \" \" + \" \".join(row.selected_text.lower().split())\n",
    "\n",
    "        len_st = len(selected_text) - 1\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "\n",
    "        for ind in (i for i, e in enumerate(tweet) if e == selected_text[1]):\n",
    "            if \" \" + tweet[ind: ind + len_st] == selected_text:\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_st - 1\n",
    "                break\n",
    "\n",
    "        char_targets = [0] * len(tweet)\n",
    "        if idx0 != None and idx1 != None:\n",
    "            for ct in range(idx0, idx1 + 1):\n",
    "                char_targets[ct] = 1\n",
    "\n",
    "        target_idx = []\n",
    "        for j, (offset1, offset2) in enumerate(offsets):\n",
    "            if sum(char_targets[offset1: offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "\n",
    "        start_idx = target_idx[0]\n",
    "        end_idx = target_idx[-1]\n",
    "\n",
    "        return start_idx, end_idx\n",
    "\n",
    "    def row_to_tensor(self, row):\n",
    "        tweet = \" \" + \" \".join(row.text.lower().split())\n",
    "        encoding = self.tokenizer.encode(tweet)\n",
    "        sentiment_id = self.tokenizer.encode(row.sentiment).ids\n",
    "        ids = [0] + sentiment_id + [2, 2] + encoding.ids + [2]\n",
    "        token_type_ids = [0, 0, 0, 0] + [0] * (len(encoding.ids) + 1)\n",
    "        offsets = [(0, 0)] * 4 + encoding.offsets + [(0, 0)]\n",
    "\n",
    "        pad_len = config.max_seq_len - len(ids)\n",
    "        if pad_len > 0:\n",
    "            ids += [1] * pad_len\n",
    "            token_type_ids = token_type_ids + ([0] * pad_len)\n",
    "            offsets += [(0, 0)] * pad_len\n",
    "\n",
    "        ids = torch.tensor(ids)\n",
    "        attention_mask = torch.where(ids != 1, torch.tensor(1), torch.tensor(0))\n",
    "        token_type_ids = torch.tensor(token_type_ids)\n",
    "        offsets = torch.tensor(offsets)\n",
    "\n",
    "        data = dict()\n",
    "        data['tweet'] = tweet\n",
    "        data['ids'] = ids\n",
    "        data['attention_mask'] = attention_mask\n",
    "        data['token_type_ids'] = token_type_ids\n",
    "        data['offsets'] = offsets\n",
    "        return data\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # return self.x_data[index], self.y_data[index]\n",
    "        return self.data[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        # return len(self.y_data)\n",
    "        return len(self.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils/model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "#     \"\"\"\n",
    "#     Return the sum of the cross entropy losses for both the start and end logits\n",
    "#     \"\"\"\n",
    "#     loss_fct = nn.CrossEntropyLoss()\n",
    "#     start_loss = loss_fct(start_logits, start_positions)\n",
    "#     end_loss = loss_fct(end_logits, end_positions)\n",
    "#     total_loss = (start_loss + end_loss)\n",
    "#     return total_loss\n",
    "\n",
    "def dist_between(start_logits, end_logits):\n",
    "    \"\"\"get dist btw. pred & ground_truth\"\"\"\n",
    "    linear_func = torch.tensor(np.linspace(0, 1, config.max_seq_len, endpoint=False), requires_grad=False).float()\n",
    "    linear_func = linear_func.to(config.device)\n",
    "\n",
    "    # 版本bug, 相乘需要类型一致\n",
    "    start_pos = (start_logits * linear_func).sum(axis=1)\n",
    "    end_pos = (end_logits * linear_func).sum(axis=1)\n",
    "\n",
    "    diff = end_pos - start_pos\n",
    "\n",
    "    return diff.sum(axis=0) / diff.size(0)\n",
    "\n",
    "\n",
    "def calc_dist_loss(start_logits, end_logits, start_positions, end_positions, scale=1):\n",
    "    # calculate distance loss between prediction's length & GT's length\n",
    "    start_logits = torch.nn.Softmax(1)(start_logits)\n",
    "    end_logits = torch.nn.Softmax(1)(end_logits)\n",
    "\n",
    "    start_one_hot = one_hot(start_positions, num_classes=config.max_seq_len).to(config.device).float()\n",
    "    end_one_hot = one_hot(end_positions, num_classes=config.max_seq_len).to(config.device).float()\n",
    "    pred_dist = dist_between(start_logits, end_logits)\n",
    "    gt_dist = dist_between(start_one_hot, end_one_hot)  # always positive\n",
    "    diff = (gt_dist - pred_dist)\n",
    "    # as diff is smaller, make it get closer to the one\n",
    "    rev_diff_squared = 1 - torch.sqrt(diff * diff)\n",
    "    # by using negative log function, if argument is near zero -> inifinite, near one -> zero\n",
    "    loss = -torch.log(rev_diff_squared)\n",
    "\n",
    "    return loss * scale\n",
    "\n",
    "\n",
    "def loss_fn(start_logits, end_logits, start_positions, end_positions):\n",
    "    start_loss = torch.nn.CrossEntropyLoss()(start_logits, start_positions)\n",
    "    end_loss = torch.nn.CrossEntropyLoss()(end_logits, end_positions)\n",
    "\n",
    "    idx_loss = (start_loss + end_loss)\n",
    "\n",
    "    dist_loss = calc_dist_loss(\n",
    "        start_logits, end_logits,\n",
    "        start_positions, end_positions)\n",
    "\n",
    "    total_loss = idx_loss + dist_loss\n",
    "    return total_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# model/bert"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model/roberta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaModel, RobertaTokenizer, RobertaConfig\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        self.config = RobertaConfig.from_pretrained(model_config.pretrain_model_path)\n",
    "        self.config.output_hidden_states = True\n",
    "        self.model = RobertaModel.from_pretrained(model_config.pretrain_model_path, config=self.config)\n",
    "        self.config.output_hidden_states = True\n",
    "        self.dropout = nn.Dropout(self.config.hidden_dropout_prob)\n",
    "        self.classifier = nn.Linear(self.config.hidden_size * 2, 2)\n",
    "        torch.nn.init.normal_(self.classifier.weight, std=0.02)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask=None, token_type_ids=None, labels=None,\n",
    "                position_ids=None, head_mask=None):\n",
    "        print(input_ids.shape)\n",
    "        print(attention_mask.shape)\n",
    "        print(token_type_ids.shape)\n",
    "        outputs = self.model(\n",
    "            input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "        hidden_states = outputs[2]\n",
    "        # out = torch.cat((hidden_states[-1], hidden_states[-2]), dim=-1)\n",
    "        out = torch.stack([hidden_states[-1], hidden_states[-2], hidden_states[-3]])\n",
    "        out = torch.mean(out, 0)\n",
    "        out = self.dropout(out)\n",
    "        print(out.shape)\n",
    "        logits = self.classifier(out)\n",
    "        print(logits.shape)\n",
    "        start_logits, end_logits = logits.split(1, dim=-1)\n",
    "        print(start_logits.shape)\n",
    "        start_logits = start_logits.squeeze(-1) \n",
    "        end_logits = end_logits.squeeze(-1) \n",
    "\n",
    "        return start_logits, end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train:90, val:10\n",
      "start\n",
      "epoch:1, step:3\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128, 1536])\n",
      "torch.Size([32, 128, 2])\n",
      "torch.Size([32, 128, 1])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128, 1536])\n",
      "torch.Size([32, 128, 2])\n",
      "torch.Size([32, 128, 1])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128, 1536])\n",
      "torch.Size([26, 128, 2])\n",
      "torch.Size([26, 128, 1])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128, 1536])\n",
      "torch.Size([10, 128, 2])\n",
      "torch.Size([10, 128, 1])\n",
      "the current epoch: 1/4, val loss: 0.083, val acc: 56.87%, cost: 63s *\n",
      "epoch:2, step:3\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128, 1536])\n",
      "torch.Size([32, 128, 2])\n",
      "torch.Size([32, 128, 1])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128, 1536])\n",
      "torch.Size([32, 128, 2])\n",
      "torch.Size([32, 128, 1])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128, 1536])\n",
      "torch.Size([26, 128, 2])\n",
      "torch.Size([26, 128, 1])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128, 1536])\n",
      "torch.Size([10, 128, 2])\n",
      "torch.Size([10, 128, 1])\n",
      "the current epoch: 2/4, val loss: 0.032, val acc: 61.98%, cost: 66s *\n",
      "epoch:3, step:3\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128, 1536])\n",
      "torch.Size([32, 128, 2])\n",
      "torch.Size([32, 128, 1])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128, 1536])\n",
      "torch.Size([32, 128, 2])\n",
      "torch.Size([32, 128, 1])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128, 1536])\n",
      "torch.Size([26, 128, 2])\n",
      "torch.Size([26, 128, 1])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128, 1536])\n",
      "torch.Size([10, 128, 2])\n",
      "torch.Size([10, 128, 1])\n",
      "the current epoch: 3/4, val loss: 0.033, val acc: 61.98%, cost: 77s *\n",
      "epoch:4, step:3\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128, 1536])\n",
      "torch.Size([32, 128, 2])\n",
      "torch.Size([32, 128, 1])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128])\n",
      "torch.Size([32, 128, 1536])\n",
      "torch.Size([32, 128, 2])\n",
      "torch.Size([32, 128, 1])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128])\n",
      "torch.Size([26, 128, 1536])\n",
      "torch.Size([26, 128, 2])\n",
      "torch.Size([26, 128, 1])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128])\n",
      "torch.Size([10, 128, 1536])\n",
      "torch.Size([10, 128, 2])\n",
      "torch.Size([10, 128, 1])\n",
      "the current epoch: 4/4, val loss: 0.058, val acc: 60.31%, cost: 67s \n"
     ]
    }
   ],
   "source": [
    "model_name = model_config.model_name\n",
    "tokenizer = model_config.tokenizer\n",
    "\n",
    "\n",
    "# def get_inputs(batch_x, batch_y=None):\n",
    "#     if model_name in ['bert', \"roberta\", 'xlmroberta']:\n",
    "#         batch_x = tuple(t.to(device) for t in batch_x)\n",
    "#         inputs = dict(input_ids=batch_x[0], attention_mask=batch_x[1])\n",
    "#         if model_name in [\"bert\"]:\n",
    "#             inputs['token_type_ids'] = batch_x[2]\n",
    "#         return inputs\n",
    "#     else:\n",
    "#         return dict(input_ids=batch_x.to(device))\n",
    "\n",
    "\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    data_len = 0\n",
    "    total_loss = 0\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "    with torch.no_grad():\n",
    "        for inputs in val_loader:\n",
    "            batch_len = len(inputs)\n",
    "            data_len += batch_len\n",
    "            tweet = inputs[\"tweet\"]\n",
    "            selected_text = inputs[\"selected_text\"]\n",
    "            sentiment = inputs[\"sentiment\"]\n",
    "            ids = inputs[\"ids\"]\n",
    "            token_type_ids = inputs[\"token_type_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            targets_start_idx = inputs[\"start_idx\"]\n",
    "            targets_end_idx = inputs[\"end_idx\"]\n",
    "            offsets = inputs[\"offsets\"]\n",
    "            # Move ids, masks, and targets to gpu while setting as torch.long\n",
    "            ids = ids.to(config.device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(config.device, dtype=torch.long)\n",
    "            attention_mask = attention_mask.to(config.device, dtype=torch.long)\n",
    "            targets_start_idx = targets_start_idx.to(config.device, dtype=torch.long)\n",
    "            targets_end_idx = targets_end_idx.to(config.device, dtype=torch.long)\n",
    "            start_logits, end_logits = model(\n",
    "                input_ids=ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "            )\n",
    "            loss = loss_fn(start_logits, end_logits, targets_start_idx, targets_end_idx)\n",
    "            total_loss += loss.item() * batch_len\n",
    "            pred_start_idxs = torch.argmax(start_logits, dim=1).cpu().data.numpy()\n",
    "            pred_end_idxs = torch.argmax(end_logits, dim=1).cpu().data.numpy()\n",
    "            for i in range(len(tweet)):\n",
    "                y_true_list.append((tweet[i], selected_text[i], sentiment[i], offsets[i]))\n",
    "                y_pred_list.append((pred_start_idxs[i], pred_end_idxs[i]))\n",
    "    return total_loss / data_len, get_score(y_true_list, y_pred_list)\n",
    "\n",
    "\n",
    "def train(train_data, val_data, fold_idx=None):\n",
    "    train_dataset = MyDataset(train_data, tokenizer)\n",
    "    val_dataset = MyDataset(val_data, tokenizer)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=config.batch_size)\n",
    "\n",
    "    model = Model().to(config.device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=model_config.learning_rate)\n",
    "    # scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.1)\n",
    "\n",
    "    if fold_idx is None:\n",
    "        print('start')\n",
    "        model_save_path = os.path.join(config.model_path, '{}.bin'.format(model_name))\n",
    "    else:\n",
    "        print('start fold: {}'.format(fold_idx + 1))\n",
    "        model_save_path = os.path.join(config.model_path, '{}_fold{}.bin'.format(model_name, fold_idx))\n",
    "\n",
    "    best_val_score = 0\n",
    "    last_improved_epoch = 0\n",
    "    adjust_lr_num = 0\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "    for cur_epoch in range(config.epochs_num):\n",
    "        start_time = int(time.time())\n",
    "        model.train()\n",
    "        print('epoch:{}, step:{}'.format(cur_epoch + 1, len(train_loader)))\n",
    "        cur_step = 0\n",
    "        # for batch_x, batch_y in train_loader:\n",
    "        for inputs in train_loader:\n",
    "            tweet = inputs[\"tweet\"]\n",
    "            selected_text = inputs[\"selected_text\"]\n",
    "            sentiment = inputs[\"sentiment\"]\n",
    "            ids = inputs[\"ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            token_type_ids = inputs[\"token_type_ids\"]\n",
    "            targets_start_idx = inputs[\"start_idx\"]\n",
    "            targets_end_idx = inputs[\"end_idx\"]\n",
    "            offsets = inputs[\"offsets\"]\n",
    "            # Move ids, masks, and targets to gpu while setting as torch.long\n",
    "            ids = ids.to(config.device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(config.device, dtype=torch.long)\n",
    "            attention_mask = attention_mask.to(config.device, dtype=torch.long)\n",
    "            targets_start_idx = targets_start_idx.to(config.device, dtype=torch.long)\n",
    "            targets_end_idx = targets_end_idx.to(config.device, dtype=torch.long)\n",
    "            optimizer.zero_grad()\n",
    "            start_logits, end_logits = model(\n",
    "                input_ids=ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "            )\n",
    "            loss = loss_fn(start_logits, end_logits, targets_start_idx, targets_end_idx)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            cur_step += 1\n",
    "            # pred_start_idxs = torch.argmax(start_logits, dim=1).cpu().data.numpy()\n",
    "            # pred_end_idxs = torch.argmax(end_logits, dim=1).cpu().data.numpy()\n",
    "            # for i in range(len(tweet)):\n",
    "            #     y_true_list.append((tweet[i], selected_text[i], sentiment[i], offsets[i]))\n",
    "            #     y_pred_list.append((pred_start_idxs[i], pred_end_idxs[i]))\n",
    "            if cur_step % config.train_print_step == 0:\n",
    "                msg = 'the current step: {0}/{1}, cost: {2}s'\n",
    "                print(msg.format(cur_step, len(train_loader), int(time.time()) - start_time))\n",
    "            #     train_score = get_score(y_true_list, y_pred_list)\n",
    "            #     msg = 'the current step: {0}/{1}, train score: {2:>6.2%}'\n",
    "            #     print(msg.format(cur_step, len(train_loader), train_score))\n",
    "            #     y_true_list = []\n",
    "            #     y_pred_list = []\n",
    "        val_loss, val_score = evaluate(model, val_loader)\n",
    "        if val_score >= best_val_score:\n",
    "            best_val_score = val_score\n",
    "            torch.save(model.state_dict(), model_save_path)\n",
    "            improved_str = '*'\n",
    "            last_improved_epoch = cur_epoch\n",
    "        else:\n",
    "            improved_str = ''\n",
    "        msg = 'the current epoch: {0}/{1}, val loss: {2:>5.2}, val acc: {3:>6.2%}, cost: {4}s {5}'\n",
    "        end_time = int(time.time())\n",
    "        print(msg.format(cur_epoch + 1, config.epochs_num, val_loss, val_score,\n",
    "                         end_time - start_time, improved_str))\n",
    "        if cur_epoch - last_improved_epoch >= config.patience_epoch:\n",
    "            if adjust_lr_num >= model_config.adjust_lr_num:\n",
    "                print(\"No optimization for a long time, auto stopping...\")\n",
    "                break\n",
    "            print(\"No optimization for a long time, adjust lr...\")\n",
    "            last_improved_epoch = cur_epoch  # 加上，不然会连续更新的\n",
    "            adjust_lr_num += 1\n",
    "\n",
    "    del model\n",
    "    gc.collect()\n",
    "\n",
    "    if fold_idx is not None:\n",
    "        model_score[fold_idx] = best_val_score\n",
    "\n",
    "        \n",
    "def predict():\n",
    "    model = Model().to(config.device)\n",
    "    model_save_path = os.path.join(config.model_path, '{}.bin'.format(model_name))\n",
    "    model.load_state_dict(torch.load(model_save_path))\n",
    "\n",
    "    test_df = pd.read_csv(config.test_path)\n",
    "    test_df.loc[:, 'selected_text'] = test_df.text.values\n",
    "    submission = pd.read_csv(config.sample_submission_path)\n",
    "\n",
    "    test_dataset = MyDataset(test_df, tokenizer, 'test')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
    "\n",
    "    pred_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            tweet = inputs[\"tweet\"]\n",
    "            sentiment = inputs[\"sentiment\"]\n",
    "            ids = inputs[\"ids\"]\n",
    "            token_type_ids = inputs[\"token_type_ids\"]\n",
    "            mask = inputs[\"mask\"]\n",
    "            offsets = inputs[\"offsets\"]\n",
    "            ids = ids.to(config.device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(config.device, dtype=torch.long)\n",
    "            mask = mask.to(config.device, dtype=torch.long)\n",
    "            start_logits, end_logits = model(\n",
    "                input_ids=ids, attention_mask=mask, token_type_ids=token_type_ids,\n",
    "            )\n",
    "            pred_start_idx = torch.argmax(start_logits, dim=1).cpu().data.numpy()\n",
    "            pred_end_idx = torch.argmax(end_logits, dim=1).cpu().data.numpy()\n",
    "\n",
    "            for i in range(len(tweet)):\n",
    "                pred = get_selected_text(tweet[i], sentiment[i], pred_start_idx[i], pred_end_idx[i], offsets[i])\n",
    "                pred_list.append(pred)\n",
    "    submission['selected_text'] = pred_list\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "def main():\n",
    "    train_df = pd.read_csv(config.train_path)\n",
    "    train_df = train_df[pd.notnull(train_df['text'])]\n",
    "    train_df = train_df[:100]\n",
    "    mode = 2\n",
    "    if mode == 1:\n",
    "        x = train_df['text'].values\n",
    "        y = train_df['sentiment'].values\n",
    "        skf = StratifiedKFold(n_splits=config.n_splits, random_state=0, shuffle=True)\n",
    "        for fold_idx, (train_idx, val_idx) in enumerate(skf.split(x, y)):\n",
    "            train(train_df.iloc[train_idx], train_df.iloc[val_idx], fold_idx)\n",
    "        score = 0\n",
    "        score_list = []\n",
    "        for fold_idx in range(config.n_splits):\n",
    "            score += model_score[fold_idx]\n",
    "            score_list.append('{:.4f}'.format(model_score[fold_idx]))\n",
    "        print('val score:{}, avg val score:{:.4f}'.format(','.join(score_list), score / config.n_splits))\n",
    "    else:\n",
    "        train_data, val_data = train_test_split(train_df, test_size=0.1, random_state=0, shuffle=True)\n",
    "        print('train:{}, val:{}'.format(train_data.shape[0], val_data.shape[0]))\n",
    "        train(train_data, val_data)\n",
    "    \n",
    "config.batch_size = 32\n",
    "config.epochs_num = 4\n",
    "model_score = dict()\n",
    "main()\n",
    "# predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/Users/yphacker/yphacker/kaggle/tweet-sentiment-extraction/model/roberta_fold0.bin'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-1096a65cf4e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0msubmission\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'submission.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-18-1096a65cf4e5>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m()\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0mmodel_save_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'{}_fold{}.bin'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfold_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_save_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     94\u001b[0m         \u001b[0my_true_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;31m#         submission = pd.DataFrame(pred_list)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/envs/py36/lib/python3.6/site-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    417\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 419\u001b[0;31m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    420\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpathlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    421\u001b[0m         \u001b[0mnew_fd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/Users/yphacker/yphacker/kaggle/tweet-sentiment-extraction/model/roberta_fold0.bin'"
     ]
    }
   ],
   "source": [
    "def model_predict(model, test_loader):\n",
    "    y_true_list = []\n",
    "    y_pred_list = []\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for inputs in test_loader:\n",
    "            tweet = inputs[\"tweet\"]\n",
    "            sentiment = inputs[\"sentiment\"]\n",
    "            ids = inputs[\"ids\"]\n",
    "            token_type_ids = inputs[\"token_type_ids\"]\n",
    "            attention_mask = inputs[\"attention_mask\"]\n",
    "            offsets = inputs[\"offsets\"]\n",
    "            ids = ids.to(config.device, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(config.device, dtype=torch.long)\n",
    "            attention_mask = attention_mask.to(config.device, dtype=torch.long)\n",
    "            start_logits, end_logits = model(\n",
    "                input_ids=ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n",
    "            )\n",
    "            pred_start_probs = torch.softmax(start_logits, dim=1).cpu().data.numpy()\n",
    "            pred_end_probs = torch.softmax(end_logits, dim=1).cpu().data.numpy()\n",
    "            for i in range(len(tweet)):\n",
    "                y_true_list.append((tweet[i], sentiment[i], offsets[i]))\n",
    "                y_pred_list.append((pred_start_probs[i], pred_end_probs[i]))\n",
    "    return y_true_list, y_pred_list\n",
    "    \n",
    "def get_pred_list(y_true_list, preds_dict):\n",
    "    data_len = len(y_true_list)\n",
    "    y_pred_list = []\n",
    "    mode = 2\n",
    "    if mode == 1:\n",
    "        for i in range(data_len):\n",
    "            preds = []\n",
    "            for fold_idx in range(config.n_splits):\n",
    "                prob = preds_dict['{}'.format(fold_idx)][i]\n",
    "                pred = np.argmax(prob)\n",
    "                preds.append(pred)\n",
    "            # pred_set = set([x for x in preds])\n",
    "            label_id = max(preds, key=preds.count)\n",
    "            y_pred_list.append(label_id)\n",
    "    else:\n",
    "        for i in range(data_len):\n",
    "            start_logits, end_logits = None, None\n",
    "            for fold_idx in range(config.n_splits):\n",
    "                if start_logits is None:\n",
    "                    start_logits, end_logits = preds_dict['{}'.format(fold_idx)][i]\n",
    "                else:\n",
    "                    tmp1, tmp2 = preds_dict['{}'.format(fold_idx)][i]\n",
    "                    start_logits += tmp1\n",
    "                    end_logits += tmp2\n",
    "            y_pred_list.append((np.argmax(start_logits), np.argmax(end_logits)))\n",
    "            \n",
    "    pred_list = []\n",
    "    for y_true, y_pred in zip(y_true_list, y_pred_list):\n",
    "        text = y_true[0]\n",
    "        sentiment = y_true[1]\n",
    "        offsets = y_true[2]\n",
    "        pred_start_idx = y_pred[0]\n",
    "        pred_end_idx = y_pred[1]\n",
    "        pred = get_selected_text(text, sentiment, pred_start_idx, pred_end_idx, offsets)\n",
    "        pred_list.append(pred)\n",
    "    return pred_list\n",
    "\n",
    "def predict():\n",
    "    test_df = pd.read_csv(config.test_path)\n",
    "    test_df = test_df[:100]\n",
    "    test_df.loc[:, 'selected_text'] = test_df.text.values\n",
    "\n",
    "    test_dataset = MyDataset(test_df, tokenizer, 'test')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size)\n",
    "\n",
    "    preds_dict = dict()\n",
    "    y_true_list = []\n",
    "    for fold_idx in range(config.n_splits):\n",
    "        model = Model().to(config.device)\n",
    "        model_save_path = os.path.join(config.model_path, '{}_fold{}.bin'.format(model_name, fold_idx))\n",
    "        model.load_state_dict(torch.load(model_save_path))\n",
    "        y_true_list, y_pred_list = model_predict(model, test_loader)\n",
    "#         submission = pd.DataFrame(pred_list)\n",
    "#         submission.to_csv('{}/{}_fold{}_submission.csv'\n",
    "#                           .format(config.submission_path, model_name, fold_idx), index=False, header=False)\n",
    "        preds_dict['{}'.format(fold_idx)] = y_pred_list\n",
    "    pred_list = get_pred_list(y_true_list, preds_dict)\n",
    "\n",
    "    submission = pd.read_csv(config.sample_submission_path)\n",
    "    submission = submission[:100]\n",
    "    submission['selected_text'] = pred_list\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "predict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}