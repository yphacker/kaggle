{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://www.kaggle.com/benjaminkz/quora-question-pairs-xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../input/quora-question-pairs-feature-extraction-2/train.csv\")\n",
    "test = pd.read_csv(\"../input/quora-question-pairs-feature-extraction-2/test.csv\")\n",
    "trainlabel = pd.read_csv(\"../input/quora-question-pairs-feature-extraction-2/trainlabel.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtrain = xgb.DMatrix(train, label = trainlabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = 0.369197853026293\n",
    "pos_public = (0.55410 + np.log(1 - p)) / np.log((1 - p) / p)\n",
    "pos_private = (0.55525 + np.log(1 - p)) / np.log((1 - p) / p)\n",
    "average = (pos_public + pos_private) / 2\n",
    "print (pos_public, pos_private, average)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w0 = average * (1 - p) / ((1 - average) * p)\n",
    "print(w0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = average / p\n",
    "w2 = (1 - average) / (1 - p)\n",
    "print(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_log_loss(preds, dtrain):\n",
    "    label = dtrain.get_label()\n",
    "    return \"weighted_logloss\", -np.mean(w1 * label * np.log(preds) + w2 * (1 - label) * np.log(1 - preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = \"logloss\"\n",
    "params[\"eta\"] = 0.1\n",
    "params[\"max_depth\"] = 6\n",
    "params[\"min_child_weight\"] = 1\n",
    "params[\"gamma\"] = 0\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.9\n",
    "params[\"scale_pos_weight\"] = 0.3632\n",
    "params[\"tree_method\"] = \"gpu_hist\"  # 使用GPU加速的直方图算法\n",
    "params['max_bin'] = 256\n",
    "\n",
    "model1 = xgb.cv(params, dtrain, num_boost_round = 2000, nfold = 10, \n",
    "                feval = weighted_log_loss, early_stopping_rounds = 200, \n",
    "                verbose_eval = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.1\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"subsample\"] = 0.8\n",
    "fix_params[\"colsample_bytree\"] = 0.9\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_bin\"] = 256\n",
    "\n",
    "evaluation_list = []\n",
    "for depth in [5, 6]:\n",
    "    for child_weight in [1, 2.5, 4]:\n",
    "        params = {**fix_params, **{\"max_depth\": depth, \"min_child_weight\": child_weight}}\n",
    "        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                            feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "        # evaluation记录了每一轮迭代的交叉验证结果\n",
    "        evaluation_list.append(evaluation)\n",
    "        \n",
    "for depth in [7, 8]:\n",
    "    for child_weight in [4, 5, 6]:\n",
    "        params = {**fix_params, **{\"max_depth\": depth, \"min_child_weight\": child_weight}}\n",
    "        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                            feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "        # evaluation记录了每一轮迭代的交叉验证结果\n",
    "        evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    # evaluation的最后一行即相应参数组合的结果\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.1\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"subsample\"] = 0.8\n",
    "fix_params[\"colsample_bytree\"] = 0.9\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_bin\"] = 256\n",
    "\n",
    "evaluation_list = []\n",
    "for depth in [5, 6, 7]:\n",
    "    for child_weight in [3, 3.5, 4, 4.5]:\n",
    "        params = {**fix_params, **{\"max_depth\": depth, \"min_child_weight\": child_weight}}\n",
    "        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                            feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "        evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.1\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"subsample\"] = 0.8\n",
    "fix_params[\"colsample_bytree\"] = 0.9\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_depth\"] = 6\n",
    "fix_params[\"min_child_weight\"] = 4\n",
    "\n",
    "evaluation_list = []\n",
    "for bin in [200, 230, 256, 280]:\n",
    "    params = {**fix_params, **{\"max_bin\": bin}}\n",
    "    evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                        feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "    evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.08\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"subsample\"] = 0.8\n",
    "fix_params[\"colsample_bytree\"] = 0.9\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_depth\"] = 6\n",
    "fix_params[\"min_child_weight\"] = 3.5\n",
    "\n",
    "evaluation_list = []\n",
    "for bin in [220, 240, 270]:\n",
    "    params = {**fix_params, **{\"max_bin\": bin}}\n",
    "    evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                        feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "    evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.1\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_depth\"] = 6\n",
    "fix_params[\"min_child_weight\"] = 4\n",
    "fix_params[\"max_bin\"] = 256\n",
    "\n",
    "evaluation_list = []\n",
    "for row in [0.7, 0.8, 0.9]:\n",
    "    for col in [0.7, 0.8, 0.9]:\n",
    "        params = {**fix_params, **{\"subsample\": row, \"colsample_bytree\": col}}\n",
    "        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                            feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "        evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fix_params = {}\n",
    "fix_params[\"objective\"] = \"binary:logistic\"\n",
    "fix_params[\"eval_metric\"] = \"logloss\"\n",
    "fix_params[\"eta\"] = 0.1\n",
    "fix_params[\"gamma\"] = 0\n",
    "fix_params[\"scale_pos_weight\"] = 0.3632\n",
    "fix_params[\"tree_method\"] = \"gpu_hist\"\n",
    "fix_params[\"max_depth\"] = 6\n",
    "fix_params[\"min_child_weight\"] = 4\n",
    "fix_params[\"max_bin\"] = 256\n",
    "\n",
    "evaluation_list = []\n",
    "for row in [0.75, 0.8, 0.85]:\n",
    "    for col in [0.85, 0.9]:\n",
    "        params = {**fix_params, **{\"subsample\": row, \"colsample_bytree\": col}}\n",
    "        evaluation = xgb.cv(params, dtrain, num_boost_round = 650, nfold = 6, \n",
    "                            feval = weighted_log_loss, early_stopping_rounds = 100)\n",
    "        evaluation_list.append(evaluation)\n",
    "\n",
    "evaluation_panel = pd.DataFrame()\n",
    "for evaluation in evaluation_list:\n",
    "    evaluation_panel = pd.concat([evaluation_panel, evaluation.iloc[-1, :]], axis = 1)\n",
    "evaluation_panel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = \"logloss\"\n",
    "params[\"eta\"] = 0.06\n",
    "params[\"gamma\"] = 0\n",
    "params[\"scale_pos_weight\"] = 0.3632\n",
    "params[\"tree_method\"] = \"gpu_hist\"\n",
    "params[\"max_depth\"] = 6\n",
    "params[\"min_child_weight\"] = 4\n",
    "params[\"max_bin\"] = 256\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.9\n",
    "\n",
    "model6 = xgb.cv(params, dtrain, num_boost_round = 6000, nfold = 10, \n",
    "                feval = weighted_log_loss, early_stopping_rounds = 150, \n",
    "                verbose_eval = 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = \"logloss\"\n",
    "params[\"eta\"] = 0.04\n",
    "params[\"gamma\"] = 0\n",
    "params[\"scale_pos_weight\"] = 0.3632\n",
    "params[\"tree_method\"] = \"gpu_hist\"\n",
    "params[\"max_depth\"] = 6\n",
    "params[\"min_child_weight\"] = 4\n",
    "params[\"max_bin\"] = 256\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.9\n",
    "\n",
    "model4 = xgb.cv(params, dtrain, num_boost_round = 6000, nfold = 10, \n",
    "                feval = weighted_log_loss, early_stopping_rounds = 150, \n",
    "                verbose_eval = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = \"logloss\"\n",
    "params[\"eta\"] = 0.02\n",
    "params[\"gamma\"] = 0\n",
    "params[\"scale_pos_weight\"] = 0.3632\n",
    "params[\"tree_method\"] = \"gpu_hist\"\n",
    "params[\"max_depth\"] = 6\n",
    "params[\"min_child_weight\"] = 4\n",
    "params[\"max_bin\"] = 256\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.9\n",
    "\n",
    "model2 = xgb.cv(params, dtrain, num_boost_round = 6000, nfold = 10, \n",
    "                feval = weighted_log_loss, early_stopping_rounds = 150, \n",
    "                verbose_eval = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {}\n",
    "params[\"objective\"] = \"binary:logistic\"\n",
    "params[\"eval_metric\"] = \"logloss\"\n",
    "params[\"eta\"] = 0.02\n",
    "params[\"gamma\"] = 0\n",
    "params[\"scale_pos_weight\"] = 0.3632\n",
    "params[\"tree_method\"] = \"gpu_hist\"\n",
    "params[\"max_depth\"] = 6\n",
    "params[\"min_child_weight\"] = 4\n",
    "params[\"max_bin\"] = 256\n",
    "params[\"subsample\"] = 0.8\n",
    "params[\"colsample_bytree\"] = 0.9\n",
    "\n",
    "dtest = xgb.DMatrix(test)\n",
    "\n",
    "t = pd.read_csv(\"../input/quora-question-pairs/test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(params, dtrain, num_boost_round = 3600)\n",
    "prediction = model.predict(dtest)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = t[\"test_id\"]\n",
    "sub['is_duplicate'] = prediction\n",
    "sub.to_csv('submission3600.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(params, dtrain, num_boost_round = 3800)\n",
    "prediction = model.predict(dtest)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = t[\"test_id\"]\n",
    "sub['is_duplicate'] = prediction\n",
    "sub.to_csv('submission3800.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = xgb.train(params, dtrain, num_boost_round = 4100)\n",
    "prediction = model.predict(dtest)\n",
    "\n",
    "sub = pd.DataFrame()\n",
    "sub['test_id'] = t[\"test_id\"]\n",
    "sub['is_duplicate'] = prediction\n",
    "sub.to_csv('submission4100.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
